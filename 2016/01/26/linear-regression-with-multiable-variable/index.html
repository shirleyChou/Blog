<!DOCTYPE html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" Linear Regression with multiable Variable &middot;  Zhou Yi" />
  	<meta property="og:site_name" content="Zhou Yi" />
  	<meta property="og:url" content="http://shirleychou.github.io/blog/2016/01/26/linear-regression-with-multiable-variable/" />

    
  	<meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2016-01-26T17:25:34&#43;08:00" />

    
    <meta property="og:article:tag" content="Machine Learning" />
    
    <meta property="og:article:tag" content="CS 229" />
    
    <meta property="og:article:tag" content="Andrew Ng" />
    
    

  <title>
     Linear Regression with multiable Variable &middot;  Zhou Yi
  </title>

    <meta name="description" content="Recording... Action!" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="http://shirleychou.github.io/blog/images/favicon.ico">
	  <link rel="apple-touch-icon" href="http://shirleychou.github.io/blog/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="http://shirleychou.github.io/blog/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="http://shirleychou.github.io/blog/css/nav.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="http://shirleychou.github.io/blog/index.xml" rel="alternate" type="application/rss+xml" title="Zhou Yi" />
      
      
    
    <meta name="generator" content="Hugo 0.14" />

    <link rel="canonical" href="http://shirleychou.github.io/blog/2016/01/26/linear-regression-with-multiable-variable/" />

    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-79101-12', 'auto');
      ga('send', 'pageview');

    </script>
    

    
</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/blog/">Blog</a>
            </li>
        
            <h3>Follow me</h3>
            <li class="nav-opened" role="presentation">
            	<a href="https://github.com/shirleyChou">Github repos</a>
            </li>
        
    </ul>
    
    
    <a class="subscribe-button icon-feed" href="http://shirleychou.github.io/blog/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">




<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">


  
      <a class="blog-logo" href="http://shirleychou.github.io/blog/"><img src="http://shirleychou.github.io/blog/images/logo.png" alt="Home" /></a>
  
  
      <a class="menu-button" href="#"><span class="burger">&#9776;</span><span class="word">Menu</span></a>
  
  </nav>
</header>



<main class="content" role="main">




  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">Linear Regression with multiable Variable</h1>
        <small>Week 2</small>

        <section class="post-meta">
        
          <time class="post-date" datetime="2016-01-26T17:25:34&#43;08:00">
            Jan 26, 2016
          </time>
        
         
          <span class="post-tag small"><a href="http://shirleychou.github.io/blog/tags/machine-learning/">#Machine Learning</a></span>
         
          <span class="post-tag small"><a href="http://shirleychou.github.io/blog/tags/cs-229/">#CS 229</a></span>
         
          <span class="post-tag small"><a href="http://shirleychou.github.io/blog/tags/andrew-ng/">#Andrew Ng</a></span>
         
        </section>
    </header>

    <section class="post-content">
      

<h3 id="multiple-features:18551688c5d64608a15d5425bef6ac07">Multiple features</h3>

<p>Multiple features equals to multiple variables.
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/mul-variable.JPG?raw=true" alt="" />
</p>

<p>And definitely, the <strong>HYPOTHESIS</strong> of linear regression would change:
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/hypothesis.JPG?raw=true" alt="" />
</p>

<p>where theta are parameters.</p>

<h3 id="gradient-descent-for-multiple-variables:18551688c5d64608a15d5425bef6ac07">Gradient descent for multiple variables</h3>

<p>So as the number of parameter grows, remember to simultaneously update theta_j for j = 0,&hellip;,n
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/grad.JPG?raw=true" alt="" />
</p>

<p>where <strong>COST FUNCTION</strong> is still the same:</p>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/cost.JPG?raw=true" alt="" />
</p>

<h3 id="gradient-descent-in-practice-i:18551688c5d64608a15d5425bef6ac07">Gradient descent in practice I</h3>

<h4 id="feature-scaling:18551688c5d64608a15d5425bef6ac07">Feature Scaling</h4>

<ul>
<li><p>The idea of Feature Scaling is to make sure features are on a similar scale, then gradient descents can converge more quickly.
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/feature-scaling.JPG?raw=true" alt="" />
</p></li>

<li><p>More generally, get every feature into approximately a -1 ≤ x ≤ 1 range.</p></li>
</ul>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/range.JPG?raw=true" alt="" />
</p>

<h4 id="mean-normalization:18551688c5d64608a15d5425bef6ac07">Mean normalization</h4>

<p>In addition to dividing by so that the maximum value when performing feature scaling sometimes people will also do what&rsquo;s call mean normalization, which means that replace x with x - μ to make features have approximately zero mean(Do not apply to x0 = 1)</p>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/mean-normalization.JPG?raw=true" alt="" />
</p>

<h5 id="parameters-explanation:18551688c5d64608a15d5425bef6ac07">Parameters explanation:</h5>

<ul>
<li>μ1 - the average value of x in the training set</li>
<li>s1 - the range(max - min) of values of that feature</li>
</ul>

<h3 id="gradient-descent-in-practice-ii:18551688c5d64608a15d5425bef6ac07">Gradient descent in practice II</h3>

<h4 id="so-in-gradient-descent-how-do-we-make-sure-gradient-descent-is-working-correctly:18551688c5d64608a15d5425bef6ac07">So in gradient descent, how do we make sure gradient descent is working correctly?</h4>

<p>Here is something we can do. We know that the job of gradient descent is to find the theta for you that, you know, hopefully minimizes the cost function j of theta. So we pluck the cost function j of theta as the gradient descent runs. And plot that maybe looks like this:</p>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/decrease.JPG?raw=true" alt="" />
</p>

<p>So what this plot is showing, <strong>is it&rsquo;s showing the value of your cost function after each iteration of gradient descent, and the gradient descent is working properly, then J of theta should decrease after every iteration</strong>.</p>

<p>In this case, we can tell gradient descent is not working
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/not-working.JPG?raw=true" alt="" />
</p>

<p>And one useful thing that this should of plot can tell you also is that by the time you&rsquo;ve gotten out to maybe between three hundred and four hundred iterations, it looks like that J of theta hasn&rsquo;t gone down much more. So it looks like that J of theta have more or less converged because your cost function isn&rsquo;t going down much more. <strong>So looking at the figure can also help you judge whether or not gradient descent has converged</strong>.</p>

<p>By the way, the number of iterations that gradient descent takes to converge for a particular application can vary a lot. <strong>So it is hard to tell how many iteration gradient descent needs to converge, and is usually by plotting this sort of plot to try to tell if gradient descent has converged</strong>.</p>

<p>It is also possible to come up with <strong>automatic convergence test</strong>, namely to have a algorithm to try to tell you if gradient descent has converged. Say we can declare convergence if J of theta decreases by less than 10^(-3) in one iteraion. But the threshold is very difficult to decide. So maybe simply plot the figure is a proper choice.</p>

<h4 id="how-to-choose-learning-rate-alpha:18551688c5d64608a15d5425bef6ac07">How to choose learning rate alpha?</h4>

<p>If alpha is too small, slow convergence. If alpha too large, J of theta may not decrease on every iteraion or may not converge.</p>

<p>So when run gradient descent, try a range of values: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 and pick the alpha seems to be causing J of theta to decrease rapidly.</p>

<h3 id="features-and-polynomial-regression:18551688c5d64608a15d5425bef6ac07">Features and polynomial regression</h3>

<p>How to get different algorithm, sometimes very powerful ones by choosing appropriate features.</p>

<p>Suppose for housing prices prediction problem, we have two features: frontage, depth. But when we apply linear regression, you don&rsquo;t necessary have to use the exactly features x1 and x2 but can create new features by yourself.
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/area.JPG?raw=true" alt="" />
</p>

<p>So sometimes by defining new features you might actually get a better model. Closing to the idea of choosing features is this idea called polynomial regression. Let&rsquo;s say your house price looks like this:
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/quad-and-cubic.JPG?raw=true" alt="" />
</p>

<p>We can apply quadratic function or cubic function, but both of these function may not be ideal. When we use cubic function, x1, x2 and x3 take a very diffierent ranges of values, and it&rsquo;s important to use feature scaling if using gradient descent.</p>

<p>So we may find another reasonable choice like this:
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/ideal.JPG?raw=true" alt="" />
</p>

<p>We do have a broad choice on feature choosing for models.</p>

<h3 id="normal-equation:18551688c5d64608a15d5425bef6ac07">Normal equation</h3>

<p>Normal equation, which for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters theta.<br />
<strong>Normal equation is a method to solve for theta analytically.i.e. we can instead just solve for the optimal value for theta all at one go</strong>. So basically one step you get to the optimal value right there.</p>

<h4 id="formula-of-normal-equation:18551688c5d64608a15d5425bef6ac07">Formula of Normal equation</h4>

<p>Using this formula <strong>does not require any feature scaling</strong>, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent.<br />
<img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/normal-equation.JPG?raw=true" alt="" />
</p>

<h4 id="intuition-of-normal-equation:18551688c5d64608a15d5425bef6ac07">Intuition of normal equation</h4>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/intuition.JPG?raw=true" alt="" />
</p>

<h4 id="when-to-use-gradient-descent-and-when-to-use-normal-equation:18551688c5d64608a15d5425bef6ac07">When to use gradient descent and when to use normal equation?</h4>

<p><img src="https://github.com/shirleyChou/blog/blob/master/static/content/post/images/andrew-ng-ml/week2/grad-and-NE.JPG?raw=true" alt="" />
</p>

<p>So if n is large, use gradient descent to avoid to pay this all in q time, but if n is relatively small, then the normal equation might give you a better way to solve the parameters.</p>

<h5 id="what-does-the-small-and-large-means:18551688c5d64608a15d5425bef6ac07">What does the small and large means?</h5>

<p>If n is in the order of a thousand, still use  normal equation. But if n is ten thousand, inverting a ten-thousand by ten-thousand matrix starts to get kind of slow. And may then to lean in the direction of gradient descent. It is hard to give a strict number. It is usually around then thousand that start to consider switching over normal equation to gradient descent.</p>

    </section>


  <footer class="post-footer">


    
    <figure class="author-image">
        <a class="img" href="http://shirleychou.github.io/blog/" style="background-image: url(http://shirleychou.github.io/blog/images/logo.png)"><span class="hidden">Zhou Yi's Picture</span></a>
    </figure>
    

    





<section class="author">
  <h4><a href="http://shirleychou.github.io/blog/">Zhou Yi</a></h4>
  
  <p>Back-end software engineer, Machine Learning and Data Mining enthusiast.</p>
  
  <div class="author-meta">
    <span class="author-location icon-location">Beijing, China</span>
    
  </div>
</section>



    
<section class="share">
  <h4>Share this post</h4>
  <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Linear%20Regression%20with%20multiable%20Variable&amp;url=http%3a%2f%2fshirleychou.github.io%2fblog%2f2016%2f01%2f26%2flinear-regression-with-multiable-variable%2f"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      <span class="hidden">Twitter</span>
  </a>
  <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fshirleychou.github.io%2fblog%2f2016%2f01%2f26%2flinear-regression-with-multiable-variable%2f"
      onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      <span class="hidden">Facebook</span>
  </a>
  <a class="icon-pinterest" style="font-size: 1.4em" href="http://pinterest.com/pin/create/button/?url=http%3a%2f%2fshirleychou.github.io%2fblog%2f2016%2f01%2f26%2flinear-regression-with-multiable-variable%2f&amp;description=Linear%20Regression%20with%20multiable%20Variable"
      onclick="window.open(this.href, 'pinterest-share','width=580,height=296');return false;">
      <span class="hidden">Pinterest</span>
  </a>
  <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=http%3a%2f%2fshirleychou.github.io%2fblog%2f2016%2f01%2f26%2flinear-regression-with-multiable-variable%2f"
     onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
      <span class="hidden">Google+</span>
  </a>
</section>



    

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'shirley';
  var disqus_url = 'http:\/\/shirleychou.github.io\/blog\/2016\/01\/26\/linear-regression-with-multiable-variable\/';
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Zhou Yi</a> Copyright (c) 2016. All rights reserved.</section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="http://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/vjeantet/hugo-theme-casper">Casper</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="http://shirleychou.github.io/blog/js/jquery.js"></script>
    <script type="text/javascript" src="http://shirleychou.github.io/blog/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="http://shirleychou.github.io/blog/js/index.js"></script>
    
</body>
</html>

